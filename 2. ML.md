서울대학교 컴퓨터공학부 김건희 교수 
#lgaimers 
# Part 1. Introduction to Machine Learning 
- T(분류 회귀 탐지), P(성능지표), E(데이터) 로 나눌 수 있다.  ![](Pasted%20image%2020250105182539.png)
- ![](Pasted%20image%2020250105182707.png)
- 일반화 
  ![](Pasted%20image%2020250105183153.png)
  트레이닝하는 목적은 '일반화' : 어떤 패턴을 배워서 어떤 새로운 데이터가 들어와도 적응하는 것. 
- 기계학습에서 가장 어려운 것 : 모델이 새로운 데이터에서도 잘 작동할 것인가? 

  ![](Pasted%20image%2020250105183600.png)
  : #NoFreeLunchTheoremforML : 어떤 하나의 알고리즘이 모든 문제에 대해 항상 좋은 성능을 보일 수 없다는 것. 

- 머신러닝의 종류 
  ![](Pasted%20image%2020250105184218.png)
  1. 감독학습, 지도학습(Supervised learning)
  2. 비감독학습, 비지도학습(Unsupervised learning)
  3. 반지도학습(Semi-supervised learning) : 지도학습과 비지도학습의 중간형태, 일부 레이블이 있는 데이터(라벨링 데이터)와 레이블이 없는 데이터(비라벨링 데이터)를 함께 사용하여 학습하는 방법
  4. 강화학습(Reinforcement learning)
## 1. 감독학습, 지도학습(Supervised learning)
: x1이 들어가면 y1이 나와야돼! 라는 식으로 정답을 쌍으로 해서 가르쳐주는 것. 
: 대표적인 작업 : 
	Classification(y가 어떤 범주형인 것)
	Regression(output이 실수로 예측이 되게 된다.)
![](Pasted%20image%2020250106145619.png)

## 2. 비감독학습, 비지도학습(Unsupervised learning)
: 학습 데이터가 x로만 구성되어 있음.
: y 데이터를 주지 않기 때문에 의도를 파악할 수 없어 일부 부분은 마음에 들지만 마음에 들지 않는 부분이 발생한다. 
: #Clustering(데이터를 비슷한 특성을 가진 그룹으로 묶는 것), 
#anomaly_detection (이상탐지, 데이터에서 정상적인 패턴과 다른 비정상적인 패턴을 구별하는 기법 ), 
#density_estimation (밀도추정, 주어진 데이터가 어떤 분포를 따르는지 추정하는 과정, 즉, 데이터가 특정 값 주변에 얼마나 자주 분포하는지 파악하는 기법) 
![](Pasted%20image%2020250106150343.png)

## 3. 반지도학습(Semi-supervised learning) 
: 지도학습과 비지도학습의 중간형태, 일부 레이블이 있는 데이터(라벨링 데이터)와 레이블이 없는 데이터(비라벨링 데이터)를 함께 사용하여 학습하는 방법
: #LU_learning : 몇몇의 데이터는 레이블을 주고, 몇몇의 데이터는 레이블을 주지 않는 경우.
: #PU_learning : 정상 클래스와 비정상 클

래스를 나눈다고 할 때, 정상 클래스의 데이터만 주는 경우 
![](Pasted%20image%2020250106151138.png)![](Pasted%20image%2020250106151413.png)
: 반지도학습을 왜 쓸까? : 빨간색 영역에 있는 마름모를 보고, 확률적으로 빨간색일 확률이 높다는 것을 알 수 있음. 

## 4. 강화학습(Reinforcement learning)
: 환경이랑 상호작용하면서 학습을 한다. 
: No supervisor, State를 보고 action을 선택하게 된다. 
: 새로운 State을 주고, 그 다음에 Reward 보상이라는 것을 주게 된다. 
: 강화학습이 어려운 이유 : reward라는 애가 바로 주어질 수도 있지만, state변화만 있고 action을 수행하고 이 과정을 여러번 주어진 다음에야 reward가 주어지는 경우도 있기 때문에,  어렵다.
![](Pasted%20image%2020250106151745.png)

## 교수님 추천 자료 
![](Pasted%20image%2020250106152444.png)


# Part 2. Bias and Variance 

## 1. 기계학습의 정의 
![](Pasted%20image%2020250106152851.png)
: 가정 : 학습 데이터가 N개이고, X는 multi dimension, Y는 +1과 -1만 존재한다.(binary classification) 
: 우리가 정의해야되는 것 : Model class : Linear model이라면 y = wTx+b 선형함수로 정의가 된다. 
: 목표 : 잘 동작하게 하는 w와 b의 값을 찾는 것. 
: Loss function : 주어진 input에 대해서 model이 예측했고, 우리가 그 input에 대한 정답을 알고 있을 때, model의 예측 값과 정답 값이 틀리면 틀릴수록 큰 값을 주는 함수. 
	: Squared Loss : 예측 값과 정답 값이 틀리면 틀릴수록 2차의 함수로 패털티를 주는 그런 loss function
	: (in Classification) 0/1 Loss : 예측된 class가 정답이랑 똑갑으면 loss는 0이고, 틀리면 loss가 1이 발생하는 loss function

## 2. 일반화
: 학습 데이터로부터 어떤 패턴을 배워서 학습 Data 때 보지 못한 새로운 data를 보더라도 일반화하여 잘 하기를 바라는 것이 학습 목표
![](Pasted%20image%2020250106154424.png)
![](Pasted%20image%2020250106165913.png)
: ML 모델의 능력은 새로운 데이터에 대해서 잘하느냐에 달림. 
: #Overfittig : 과적합 : 잘못된 일반화
	: 그림에서보면 선이 매끄럽지 않고 울퉁불퉁하잖음? 그렇게 세세한 것까지 다 맞추려고 하다보니까 정확도가 떨어지는거임. 반면에 오른쪽은 매끄럽잖음? 그걸 일반화라고 부름. 
	: 오차 관점에서는 overfitting된 모델이 더 좋을 수 있음. 하지만, 일반화 능력이 떨어질 수 있기 때문에, 경우에 따라서 또는 많은 경우에 오른쪽 model을 더 선호하게 된다. 
	: 일종의 정확도는 희생하면서 new unseen data에 대한 일반화 능력을 더 높이기 위해서 이와 같이 학습했다고 볼 수 있다. 

예시) 
![](Pasted%20image%2020250106171210.png)
![](Pasted%20image%2020250106171954.png)
: True distribution : 모든 가능한 경우에 대해서 다 표현을 하고 있는 경우, 관측 불가능. Train dataset이나 test dataset은 결국 true distribution에서 generate 혹은 sampling 됐다고 할 수 있다. 
: Train dataset, test dataset을 만들 때, 
가정 : 
	#IID : (Independent and Identically Distributed) 
		: 독립성 : 각 샘플은 서로 영향을 주지 않고 독립적으로 발생한다. 
		: 동일한 분포 : 모든 샘플이 동일한 확률 분포를 따른다. 즉, 각 샘플은 같은 종류의 확률 분포에서 나온 값들을 가진다. 
![](Pasted%20image%2020250106173025.png)
- Generalization Error 
	: Expectation : 우리가 Loss를 하나의 숫자로 요약하겠다. 
	: x와 y는 True distribution을 따르는데, 그때 loss의 평균값 혹은 기대값을 이야기한다. 
- #Underfitting : 과소적합 
  : 일반화 error < Training error 
- #Overfitting
  : 일반화 error > Training error
  : 너무 과하게 학습 data에 적합이 됐다.
  : 우리의 목표는 overfitting을 내는 것 : 왜? 적어도 학습이 되는 모델을 찾았다는 것을 의미하기 때문에, 우리가 모은 training data에 대해서 error가 어디까지 작아질 수 있느냐를 시험해본다는 것은 중요하다. 
  : 과도한 over fitting 은 문제가 된다. 
	  : training error과 validation error 같은 것을 찍어봤는데, 그 차이가 너무 크다?  그러면 문제가 되는 것. 
	  : 이 경우 Regularization이라던지, 그런 방식을 통해서 training error를 좀 손해 보면서 validation error, test error를 좀 낮추는 거라고 볼 수 있다. 
- Overfitting과 model의 용량과의 상관관계![](Pasted%20image%2020250106174353.png)
  : 7개의 데이터와 3개의 모델이 있다고 가정. 
  : 7개의 데이터를 fit하기 위해서 1번째 그림이 선형함수를 썼다고 가정한다면, 2번째 그림은 2차 함수, 3번째 그림은 9차 함수라고 가정
  : 1번째 그림 : 굴곡이 없이 직선밖에 표현을 못함. 굴곡을 표현할 수 있는 능력, 용량 자체가 없다. -> Underfitting이 난다.
  : 2번째 그림 : 적절한 모델을 쓰면 -> 잘 fitting이 된다. 
  : 3번째 그림 : 필요 이상으로 복잡한 모델을 쓴다면 Training error는 줄일 수 있지만, (모델의 용량이 올라가면 올라갈수록 training error는 무조건 줄어든다)data가 없는 구간은 널뛰게 된다.
- #오캄의면도날
  ![](Pasted%20image%2020250106175510.png)
  : 현상을 설명할 수 있는 모델이 2-3개 있다고 가정한다면, 그 경우에는 가장 간단한 설명이 맞을 확률이 높다는 것. -> 경험에 대한 것이지, 정답에 대한 내용은 아님. 
  : 예) 나무 뒤의 나무가 2개일까 1개일까? -> 확률적으로 더 간단한 설명, 박스가 1개일 확률이 높다는 것. 박스가 2개이려면 여러가지 우연이 겹쳐야 한다.(두 개의 박스가 가까이 있어야되고, 마침 그 사이를 나무 밑동이 가려야 하니까)

- 일반적으로 모델의 용량과  error에 대한 설명 
  ![](Pasted%20image%2020250106180123.png)
  : model의 용량이 높으면 높을수록 무조건 training error는 줄어든다. 
  : but, 우리의 목적은 training error를 줄이는 것이 아니라, 일반화 error(녹색)를 줄이는 것. -> 녹색 실선이 최소값인 곳을 찾아야한다. 
  : 일반화 error의 최소값을 찾는 것은 어려움 : 왜? 일반화 데이터를 찾는 것은 불가능하고 val data를 사용하여 일반화 데이터의 최소값을 예측하는 수 밖에 없기 때문 

- #Regularization : #정규화 ![](Pasted%20image%2020250106180651.png)
  : 머신러닝 모델에서 overfitting을 방지하고 일반화 성능을 향상시키기 위해 사용되는 기법, 모델의 복잡도를 제어하여 과적합을 줄이고, 더 일반화된 모델을 만들어 냄. 
  : 특정 해결책에 대해 선호하는 점, 선호하는 해결책
  : 학습 과정 동안에 최적화 문제를 푸는데, 문제를 풀려면 목적함수가 주어져야 한다. 
  : 목적 함수는 학습 data에 대해서 우리가 loss function을 정의해서 loss가 minimize되도록 정의해야 한다. 만약 이런 학습 loss만 쓴다고 그러면 과적합에 빠질 수 있기 때문에 Term을 추가하게 되는데 이것이 regularization term이 될 수 있다. 
  : regularization term에 model의 용량이 증가하면 증가할수록 값이 커지는 그런 regularization term을 씀으로써 우리가 결국에는 loss만 minimize하는 것이 아니라 model의 용량도 minimize하도록 그렇게 regularization, 목적함수를 구성 할 수 있을 것이다. 
  : 많은 경우에 우리가 목적함수에 두가지 term이 있고, 이 두 가지 term은 다른 목적으로 유래가 되었기 때문에 #Hyperparameter, #Tuning_Parameter라는 것을 준다. 
	  #parameter : 학습을 통해서 배우는 변수
	   #Hyperparameter : 우리가 줘야되는 paramter(저기서는 람다$$ \lambda $$) : 저기서는 우리의 목적함수가 두 가지 성분으로 되어있기 때문에 각 성분에 대한 상대중요성을 나타내는 역할을 한다. 

## 3. Bias/Variance Decomposition 
![](Pasted%20image%2020250106182736.png): Loss bias : 낮은 편향 : 영점이 잘 맞음 : 예측 값에 대한 평균과 True 값과의 차이
: Low variance : 낮은 분산 : 모여있음
: bias랑 variance 다 낮아야 한다. 
![](Pasted%20image%2020250106183057.png)
: Test Error = Bias + Variance 
: Bias를 올리면 variance가 내려가고, Bias를 내리면 variance가 올라간다. => 그래서 Trade-off가 발생함. 
: 모델을 선택하는 순간 trade-off에서 자유로울 수 없다는 이야기. 
: 이 둘을 다 낮추기 위해서 많이 활용되는 방식 
	: #Ensemble_Learning : 교수님) 다른 기회를 통해서 배우시길 바랍니다^^


# 요약정리 
![](Pasted%20image%2020250106183506.png)
-  Overfitting 
	: 세세한 부분도 다 fitting을 하려했다. 
	: 모델이 불안정 => Variance가 올라간다. 
	: Variance는 모델의 용량이 올라갈 수록 올라간다. 
	: Variance를 잡기 위한 좋은 방법은 학습 데이터를 많이 모으는 것이 좋다. 
- Underfitting
	: bias가 높다 -> underfitting  
	: 모델의 복잡도가 너무 낮다. 
	: 정확도가 떨어지는 모델이다. 
	: bias를 잡으려면? -> 모델의 용량을 올리는 것이 중요하다. 

# Part 3. Recent Progress of Large Language Models
![](Pasted%20image%2020250106195300.png) 
1. GPT-3 : 
   : Generative pre-trained Transformer
   : General purpose algorithm
   : 2020년 3월 출시 
   : 새로운 알고리즘을 만드는 것보다는 간단한 알고리즘을 여러개 이어붙이면 성능을 뛰어넘을 수 있기 때문에 큰 모델을 만들자! 라는 시대였음. (지금도 유효함.)
   : GPT3부터는 코드를 공개하지 않고 API를 제공했음 
![](Pasted%20image%2020250106201500.png)
2. InstructGPT(Instruct  : 지시하다. 명령하다.) - GPT-3.5
   : GPT-3 얘는 언어 이해는 잘했음. 근데 사용자가 준 지시를 잘 따르는 것은 또 다른 문제임. 
   : gpt3를 가져다가, 사람의 지시를 유용하고, 안전하게 응답을 생성할 수 있게 학습된 모델임. 
   : #RLHT 
   : Reinforcement Learning with Human Feedback (인간 피드백을 활용한 강화학습)
   : RL : 강화학습
   : HF : 인간 피드백 
   ![](Pasted%20image%2020250106201737.png)
   : GPT-3를 어떻게 가져다 썼느냐? 
   ==> 사람의 피드백으로부터 강화학습을 통해 배웠다. 
   : 야 이런 질문이 들어오면 이런 대답을 해야돼(감독학습)
   : GPT를 가지고 여러 개의 응답을 생성하게 함
   : 사람이 그 응답을 랭킹으로 만들어서 알맞는 대답을 하게 만듦. 
   ![](Pasted%20image%2020250106202544.png)
3. InstructGPT를 학습시키는 과정 
	   1. #SFT : Supervised fine-tuning
	      : GPT3(언어 이해할 뿐이지 이행할 능력x)를 가져온다. 
	      : 어느 정도 응답을 할 수 있도록, 사람을 통해서 학습을 한다. 
	2. #RM : Reward model training
	   : GPT에게 응답을 여러 개 만들게 하고, 여러 개의 응답을 랭킹을 매김. 
	   : Ranking score를 예측하는 Reward model을 만든다. 
	3. RL via PPO 
	   : 새로운 질문을 주면, 새로운 질문에 대한 응답을 만들 수 있다. 질문과 응답이 주어지면 reward model이 score를 예측하고, 이 score를 강화학습의 보상으로 활용하여 instruct GPT를 학습시킨다. 
	   : #PPO 
		   : OpenAI에서 만든 강화학습 방법 

4. ChatGPT
   : 2022년 11월 출시 
   : 그 전까지는 API call을 해야했음. chat 봇 형태로 만들어서 누구나 사용할 수 있게 만들었음. 
   : #MAU : Monthly Active Users 
	   : 한 달동안 얼마나 많은 고유 사용자가 활동했는지 측정하는 지표 
	: 이제는 뒤에 붙는 숫자가 의미가 없음. 
	: 사람들의 질문으로 계속 학습되어가는 중
	![](Pasted%20image%2020250106205231.png)
	: attack 이 많았음 : ex) 세종대왕이 맥북을 던진 이야기를 해줘. 
	: 이제는 attack이 안먹힘 

5. GPT-4
   ![](Pasted%20image%2020250106205356.png)
   : #multimodel : 여러 형태의 데이터를 인풋으로 받음
   : 이미지와 텍스트를 인풋으로 받았을때 적절한 대답을 말할 수 있도록 학습됨. 
   : 이때부터 아무런 기술적인 디테일을 공개하지 않음. 
   : 앞에 있는 N개의 토큰을 기반으로 해서 대답을 할 수 있게 됨. 
   : 사람이 보는 시험에서 좋은 점수를 받는 모델이 됨. 
   ![](Pasted%20image%2020250106205939.png)
   : 언어에 대해서도 성능이 빠르게 올라가고 있는 중

6. GPT-4의 한계 
   ![](Pasted%20image%2020250106210149.png)
   : #Hallucinate : 없는 사실을 만들어내는 경우가 많다.
   : 아무래도 확률 모델이다보니까 쉬운 질문을 했음에도 실패하는 경우가 있다. 
   : 질문을 어떻게 하느냐에 따라서 성능이 왔다갔다한다. 
   : 사람을 통해서 label을 만들었는데, 아무래도 서양인 기반이다보니까 bias가 존재한다. 
   : 지식 업데이트가 자주 일어나지 않는다. 
   : 검증 과정을 거치지 않는다.
7. Anthropic의 Claude 
   ![](Pasted%20image%2020250106212807.png)
   : OpenAI의 연구쪽 임원들이 나와서 만든 회사.

8. Google의 Bard![](Pasted%20image%2020250106213015.png)
   : 구글 검색이랑 엮어두었음. 따른 인터페이스를 만든 것이 아님. 
9. google의 pathway paLM
   ![](Pasted%20image%2020250106213111.png)
   : 언어모델을 parameter식으로 이렇게 scaling 했더니 작은 모델에서는 범위가 좁았는데, model이 커짐으로써 유머에 대한 설명도 하고, 복잡한 작업을 한다는 것을 보여주고 있다. 
   : 머지않아 api로써 활용할 수 있을 것으로 예상. 

10. Meta의 OPT & LLaMA
    : 다 공개해서 다 같이 나아가자라는 식임. 
    : 완벽하게 공개하지는 않았지만, 어느정도  가져다가 쓸 수 있을 만큼 공개를 함. 
    : Self-instruct Tuning on LLaMA ![](Pasted%20image%2020250106213456.png)
    : 회사나 학교에서 라마를 가지고 활용하는 방식으로 쓰임
    : 스탠포드 대학에서,  GPT보다는 성능이 떨어지니, 사람이 지시하고 그 지시를 이행한 data 쌍을 GPT에서 가지고와서 그걸로 LLaMA를 학습시키는 것. -> GPT보다는 성능이 떨어지나, 더 싸고 편하고 누구든지 간에 얻을 수 있다는 것이 밝혀짐. ==> 그 이후로 self-instruct tuning에 대한 연구가 활발하게 진행 중임. 
    : 
    ![](Pasted%20image%2020250106214649.png)
    : LMsys의 #Vicuna 
	    : LLaMA를 가져와서 sharegpt.com(GPT의 재미있는 응답을 공유하는 사이트)에서 50만개의 데이터를 모은다음, 학습을 해서 Vicuna 모델을 만듦. 300달러로 만들어버림. 
	    : 이 이후에 엄청나게 많은 모델들이 만들어지고있음. 
	: ![](Pasted%20image%2020250106214905.png)
	: 이거 말고도 다양한 모델이 빠르게 만들어지는 중. 
	: GPT가 나왔다고 포기하는게 아니라 기술개발을 계속 보는 것이 중요하다. 

# 결론 
  ![](Pasted%20image%2020250106215032.png):  지금의 인공지능은 data 중심의 인공지능이라고 볼 수 있음.  
  : GPT3부터 이어져오는 것처럼 Transformer decoder 기반의 model을 엄청나게 크게 만드는 형식으로 만들고 있음. 회사마다 디테일의 차이는 있으나, data 중심인건 똑같음. 
  : 회사들은 우리만을 위한 LLM을 만들 것이냐?(In-house LLM) api를 돈주고 쓸 것이냐에 대한 고민에 빠지게 된다. 큰 회사는 두가지를 가지고 가는 것이 맞긴하나 작은 회사는 고민이 될 수 밖에. 

  

   

